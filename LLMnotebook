# Cell 1: Import Required Modules
# --------------------------------
# Import required modules for parsing and RAG
import pathlib
from parse_script import parse_page_v2, Granularity
from rag_app import call_rag

# Set the LLM_AAS_TOKEN and LLM_AAS_ENDPOINT environment variables if required
import os
os.environ["LLM_AAS_TOKEN"] = "your_llm_token_here"  # Replace with your actual LLM API token
os.environ["LLM_AAS_ENDPOINT"] = "https://your_llm_endpoint_here"  # Replace with your actual LLM endpoint


# Cell 2: Parse the PDF
# ---------------------
# Parse the PDF and extract pages
def parse_pdf(pdf_path):
    pdf_path = pathlib.Path(pdf_path)
    granularity = Granularity.PAGE  # Set granularity to PAGE
    contents = parse_page_v2(path=pdf_path, granularity=granularity)
    
    if contents:
        print(f"Extracted {len(contents)} page(s) from: {pdf_path}")
        print(f"First page content:\n\n{contents[0].text}")
        return contents[0].text  # Return the first page's text
    else:
        print("No pages or text extracted.")
        return None

# Provide the path to your PDF
pdf_path = "path_to_your_pdf.pdf"  # Replace with the path to your PDF
first_page_content = parse_pdf(pdf_path)


# Cell 3: Set Up the RAG Function
# -------------------------------
# Function to call the RAG endpoint with context
def ask_llm(question, context):
    if not context:
        print("Context is empty. Ensure the PDF was parsed correctly.")
        return None
    
    # Construct the prompt
    prompt = f"""
    Can you answer the following question based on this context?

    CONTEXT:

    {context}

    QUESTION:

    {question}

    If the answer is not available in the context, return that the information is not available and do not invent things.
    """
    
    # Replace 'your_llm_model_name' with the actual LLM model name
    llm_model_name = "your_llm_model_name"  # Update with your specific model name
    response = call_rag(llm_prompt=prompt, llm_name=llm_model_name)
    return response

# Test the function with a sample question
sample_question = "What is the main topic of the document?"
response = ask_llm(sample_question, first_page_content)

# Print the LLM's response
if response:
    print("LLM Response:\n")
    print(response["choices"][0]["message"]["content"])


# Cell 4: Integration Testing
# ---------------------------
# Verify the pipeline: PDF parsing -> LLM question answering
if first_page_content:
    test_question = "What is the focus of the document?"
    llm_response = ask_llm(test_question, first_page_content)
    
    if llm_response:
        print("\nPipeline Test Result:")
        print(llm_response["choices"][0]["message"]["content"])
    else:
        print("No response from the LLM.")
else:
    print("Failed to parse PDF. Ensure the file path is correct.")
